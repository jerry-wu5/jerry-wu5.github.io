[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jerry Wu",
    "section": "",
    "text": "Current\n\nMSBA Student | UC San Diego Rady School of Management\nAspiring Data Scientist | Tech & Healthcare\nBusiness Analytics Enthusiast\n\n\n\n\nEducation\n\nMS in Business Analytics | UCSD Rady | 2025\nBS in Management Science, Minor in Data Science | UCSD | 2024"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJerry Wu\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJerry Wu\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nJerry Wu\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "Section 2: Analysis",
    "text": "Section 2: Analysis\nI analyzed the data\n\nprint (\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "blog/hw1/hw1_questions.html",
    "href": "blog/hw1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study was a natural field experiment involving 50,083 past donors to a U.S.-based civil liberties nonprofit. Participants were randomly assigned to either a control group (receiving a standard fundraising appeal) or a treatment group (receiving a letter mentioning a matching grant offer). Within the treatment group, participants were further randomly assigned to sub-treatments varying the matching ratio ($1:$1, $2:$1, $3:$1), the maximum match amount ($25,000, $50,000, $100,000, or unstated), and the suggested donation (“ask amount”) (equal to prior gift, 1.25×, or 1.5×). The experiment tested whether these pricing manipulations influenced donor behavior. While offering any match increased response rates and revenue per solicitation, larger match ratios did not produce statistically significant differences in giving. The study also explored how effects varied by geography and found greater responsiveness in “red” states (which had voted for George W. Bush in 2004). This nuanced field experiment contributed robust evidence to the demand-side economics of charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#introduction",
    "href": "blog/hw1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe study was a natural field experiment involving 50,083 past donors to a U.S.-based civil liberties nonprofit. Participants were randomly assigned to either a control group (receiving a standard fundraising appeal) or a treatment group (receiving a letter mentioning a matching grant offer). Within the treatment group, participants were further randomly assigned to sub-treatments varying the matching ratio ($1:$1, $2:$1, $3:$1), the maximum match amount ($25,000, $50,000, $100,000, or unstated), and the suggested donation (“ask amount”) (equal to prior gift, 1.25×, or 1.5×). The experiment tested whether these pricing manipulations influenced donor behavior. While offering any match increased response rates and revenue per solicitation, larger match ratios did not produce statistically significant differences in giving. The study also explored how effects varied by geography and found greater responsiveness in “red” states (which had voted for George W. Bush in 2004). This nuanced field experiment contributed robust evidence to the demand-side economics of charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#data",
    "href": "blog/hw1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\ndf = pd.read_stata('karlan_list_2007.dta')\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\n\ntest_vars = ['mrm2', 'couple', 'female', 'ave_hh_sz']\nresults = {}\n\nfor var in test_vars:\n    df_clean = df[['treatment', 'control', var]].dropna()\n\n    treatment_group = df_clean[df_clean['treatment'] == 1][var]\n    control_group = df_clean[df_clean['control'] == 1][var]\n\n    # Manual t-test\n    mean_diff = treatment_group.mean() - control_group.mean()\n    n1, n2 = len(treatment_group), len(control_group)\n    var1, var2 = treatment_group.var(ddof=1), control_group.var(ddof=1)\n    se = np.sqrt(var1 / n1 + var2 / n2)\n    t_stat = mean_diff / se\n    df_denom = (var1 / n1 + var2 / n2) ** 2\n    df_num = (var1**2) / (n1**2 * (n1 - 1)) + (var2**2) / (n2**2 * (n2 - 1))\n    df_ttest = df_denom / df_num\n    p_value_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), df_ttest))\n    # Linear regression\n    X = sm.add_constant(df_clean['treatment'])\n    y = df_clean[var]\n    model = sm.OLS(y, X).fit()\n    coef = model.params['treatment']\n    p_value_reg = model.pvalues['treatment']\n    print(\"================================================\")\n    print(f'{var} Analysis: \\n')\n    print(f'{var} Treatment mean: {treatment_group.mean()}')\n    print(f'{var} Control mean: {control_group.mean()}')\n    print(f'{var} All Mean: {df_clean[var].mean()}')\n    print('________________________________________________')\n    print('t-test: \\n')\n    print(f't-statistic: {t_stat}')\n    print(f'p-value: {p_value_ttest}')\n    print('________________________________________________')\n    print('Linear Regression: \\n')\n    print(f'Coefficient on Treatment: {coef}')\n    print(f'p-value: {p_value_reg}\\n')\n\n================================================\nmrm2 Analysis: \n\nmrm2 Treatment mean: 13.011828117981734\nmrm2 Control mean: 12.99814226643495\nmrm2 All Mean: 13.00726808034823\n________________________________________________\nt-test: \n\nt-statistic: 0.1195315522817725\np-value: 0.9048549631450833\n________________________________________________\nLinear Regression: \n\nCoefficient on Treatment: 0.013685851546779986\np-value: 0.904885973177816\n\n================================================\ncouple Analysis: \n\ncouple Treatment mean: 0.09135794896957802\ncouple Control mean: 0.0929748269737245\ncouple All Mean: 0.0918974149381833\n________________________________________________\nt-test: \n\nt-statistic: -0.5822577486767693\np-value: 0.5603971270058028\n________________________________________________\nLinear Regression: \n\nCoefficient on Treatment: -0.0016168780041463048\np-value: 0.5593646446996638\n\n================================================\nfemale Analysis: \n\nfemale Treatment mean: 0.2751509208469954\nfemale Control mean: 0.2826978395250627\nfemale All Mean: 0.27766887200849466\n________________________________________________\nt-test: \n\nt-statistic: -1.7535132542519636\np-value: 0.07952338672686232\n________________________________________________\nLinear Regression: \n\nCoefficient on Treatment: -0.007546918678066679\np-value: 0.07869095826986866\n\n================================================\nave_hh_sz Analysis: \n\nave_hh_sz Treatment mean: 2.4300146102905273\nave_hh_sz Control mean: 2.427002429962158\nave_hh_sz All Mean: 2.4290122985839844\n________________________________________________\nt-test: \n\nt-statistic: 0.8233500123023987\np-value: 0.4103151242417935\n________________________________________________\nLinear Regression: \n\nCoefficient on Treatment: 0.003012174284715988\np-value: 0.409801160289328\n\n\n\nTo assess the randomization, I tested several baseline variables (e.g., months since last donation, gender, couple status, average household size within zip) using both t-tests and linear regressions and in every case the results from the two methods were nearly identical. None of the variables showed statistically significant differences at the 95% level, confirming balance between treatment and control groups. This supports the validity of the randomization and mirrors the role of Table 1 in the paper, which demonstrates baseline equivalence."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#experimental-results",
    "href": "blog/hw1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\ndf_bar = df[['treatment', 'control', 'gave']].dropna()\n\ndf_bar['group'] = df_bar.apply(lambda row: 'Treatment' if row['treatment'] == 1 else 'Control', axis=1)\n\ndonation_rates = df_bar.groupby('group')['gave'].mean()\n\nplt.figure(figsize=(6, 5))\nax = donation_rates.plot(kind='bar')\n\nfor i, value in enumerate(donation_rates):\n    ax.text(i, value + 0.0001, f'{value:.3f}', ha='center', va='bottom')\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Group')\nplt.ylim(0, 0.03)\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot compares donation rates between the treatment and control groups. The treatment group, which received a matching donation offer, had a higher donation rate (2.2%) than the control group (1.8%). This visual evidence suggests that the presence of a matching grant increased the likelihood of donating, consistent with the main findings in the paper.\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy import stats\n\ndf_binary = df[['treatment', 'control', 'gave']].dropna()\n\ntreatment_group = df_binary[df_binary['treatment'] == 1]['gave']\ncontrol_group = df_binary[df_binary['control'] == 1]['gave']\n\n# Manual t-test calculation\nmean_diff = treatment_group.mean() - control_group.mean()\nn1, n2 = len(treatment_group), len(control_group)\nvar1, var2 = treatment_group.var(ddof=1), control_group.var(ddof=1)\nse = np.sqrt(var1 / n1 + var2 / n2)\nt_stat = mean_diff / se\ndf_denom = (var1 / n1 + var2 / n2) ** 2\ndf_num = (var1**2) / (n1**2 * (n1 - 1)) + (var2**2) / (n2**2 * (n2 - 1))\ndf_ttest = df_denom / df_num\np_value_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), df_ttest))\n\n# Linear regression:\nX = sm.add_constant(df_binary['treatment'])\ny = df_binary['gave']\nmodel = sm.OLS(y, X).fit()\ncoef = model.params['treatment']\np_value_reg = model.pvalues['treatment']\n\nprint(\"================================================\")\nprint(f' \\'gave\\' Analysis: \\n')\nprint(f'\\'gave\\' Treatment mean: {treatment_group.mean()}')\nprint(f'\\'gave\\' Control mean: {control_group.mean()}')\nprint(f'Mean Difference: {mean_diff}')\nprint(f\"'gave' All mean: {df_binary['gave'].mean()}\")\nprint('________________________________________________')\nprint('t-test: \\n')\nprint(f't-statistic: {t_stat}')\nprint(f'p-value: {p_value_ttest}')\nprint('________________________________________________')\nprint('Linear Regression: \\n')\nprint(f'Coefficient on Treatment: {coef}')\nprint(f'p-value: {p_value_reg}')\n\n================================================\n 'gave' Analysis: \n\n'gave' Treatment mean: 0.02203856749311295\n'gave' Control mean: 0.017858212980164198\nMean Difference: 0.00418035451294875\n'gave' All mean: 0.020645728091368328\n________________________________________________\nt-test: \n\nt-statistic: 3.2094621908279835\np-value: 0.001330982345091547\n________________________________________________\nLinear Regression: \n\nCoefficient on Treatment: 0.004180354512949377\np-value: 0.001927402594901797\n\n\nTo test whether matched donations increase giving, I compared donation rates between the treatment and control groups using a t-test and a bivariate regression. The treatment group had a slightly higher donation rate (2.2% vs. 1.8%), and the difference was statistically significant in both tests. This matches results in Table 2A, Panel A of the original study and suggests that even a modest match offer can meaningfully boost donation rates. The finding highlights how small psychological nudges like matching gifts can influence charitable behavior.\n\nimport statsmodels.formula.api as smf\n\ndf_probit = df[['gave', 'treatment']].dropna()\n\nprobit_model = smf.probit('gave ~ treatment', data=df_probit).fit(disp=False)\n\nprobit_summary = probit_model.summary2().as_text()\n\nmarginal_effects = probit_model.get_margeff().summary().as_text()\n\nprint(marginal_effects)\n\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nTo replicate Table 3, Column 1 of Karlan and List (2007), I ran a probit regression with a binary outcome for donation and treatment assignment as the sole predictor. The marginal effect of treatment was 0.0043, closely matching the 0.004 reported in the paper. This confirms that the presence of a matching grant increased the probability of donating by roughly 0.4 percentage points, a statistically significant effect. While small in magnitude, the result reinforces the finding that subtle changes in perceived impact, such as matching gifts, can meaningfully influence donation behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy.stats import ttest_ind\ndf_match = df[df['treatment'] == 1][['gave', 'ratio2', 'ratio3']].dropna()\n\n# Create labels for ratio group (1:1, 2:1, 3:1)\ndef classify_ratio(row):\n    if row['ratio2'] == 1:\n        return '2:1'\n    elif row['ratio3'] == 1:\n        return '3:1'\n    else:\n        return '1:1'\n\ndf_match['match_ratio'] = df_match.apply(classify_ratio, axis=1)\n\n# Pairwise t-tests between ratios\nratios = ['1:1', '2:1', '3:1']\npairwise_results = {}\n\nfor i in range(len(ratios)):\n    for j in range(i + 1, len(ratios)):\n        group1 = df_match[df_match['match_ratio'] == ratios[i]]['gave']\n        group2 = df_match[df_match['match_ratio'] == ratios[j]]['gave']\n        t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n        print(\"================================================\")\n        print(f'{ratios[i]} vs {ratios[j]}\\n')\n        print(f't-statistic: {t_stat}')\n        print(f'p-value: {p_value}\\n')\n\n================================================\n1:1 vs 2:1\n\nt-statistic: -0.965048975142932\np-value: 0.33453078237183076\n\n================================================\n1:1 vs 3:1\n\nt-statistic: -1.0150174470156275\np-value: 0.31010856527625774\n\n================================================\n2:1 vs 3:1\n\nt-statistic: -0.05011581369764474\np-value: 0.9600305476940865\n\n\n\nTo test whether the size of the match ratio influenced donation behavior, I conducted a series of pairwise t-tests comparing response rates between the 1:1, 2:1, and 3:1 match groups. None of the differences were statistically significant at the 95% level. For example, the difference between the 2:1 and 1:1 groups yielded a p-value of 0.33, and the difference between the 3:1 and 2:1 groups had a p-value of 0.96. These results support the authors’ statement in Table 2A and on page 8 of the paper: while match offers increase giving relative to no match, larger match ratios do not provide additional benefit in terms of increasing the likelihood of donating.\n\n# Alternative: use ratio as a categorical variable\nmodel2 = smf.ols('gave ~ ratio', data=df).fit()\nmodel2_summary = model2.summary2().as_text()\nprint(model2_summary)\n\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.000      \nDependent Variable: gave             AIC:                -53252.8233\nDate:               2025-04-23 16:43 BIC:                -53217.5376\nNo. Observations:   50083            Log-Likelihood:     26630.     \nDf Model:           3                F-statistic:        3.665      \nDf Residuals:       50079            Prob (F-statistic): 0.0118     \nR-squared:          0.000            Scale:              0.020217   \n----------------------------------------------------------------------\n               Coef.    Std.Err.      t      P&gt;|t|     [0.025   0.975]\n----------------------------------------------------------------------\nIntercept      0.0179     0.0011   16.2245   0.0000    0.0157   0.0200\nratio[T.1]     0.0029     0.0017    1.6615   0.0966   -0.0005   0.0063\nratio[T.2]     0.0048     0.0017    2.7445   0.0061    0.0014   0.0082\nratio[T.3]     0.0049     0.0017    2.8016   0.0051    0.0015   0.0083\n--------------------------------------------------------------------\nOmnibus:             59812.754     Durbin-Watson:        2.005      \nProb(Omnibus):       0.000         Jarque-Bera (JB):     4316693.217\nSkew:                6.740         Prob(JB):             0.000      \nKurtosis:            46.438        Condition No.:        4          \n====================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\nTo test whether the match ratio affects donation behavior, I regressed the binary outcome gave on ratio as a categorical variable. Using the 1:1 match as the reference group, I found that the 2:1 and 3:1 match ratios had slightly higher donation rates, with coefficients of 0.0048 and 0.0049 respectively, both statistically significant at the 1% level. The 1:1 coefficient was smaller and not statistically significant. These results suggest that higher match ratios may slightly increase the likelihood of donating, although the effect is small in magnitude and inconsistent with earlier t-test results.\n\nresponse_rates = df_match.groupby('match_ratio')['gave'].mean()\n\ndiff_2v1_direct = response_rates['2:1'] - response_rates['1:1']\ndiff_3v2_direct = response_rates['3:1'] - response_rates['2:1']\n\ncoef_2v1_reg = model2.params['ratio[T.2]'] - model2.params['ratio[T.1]']\ncoef_3v2_reg = model2.params['ratio[T.3]'] - model2.params['ratio[T.2]']\n\nprint(\"Direct from data: \\n\")\nprint(f\"2:1 vs 1:1: {diff_2v1_direct}\")\nprint(f\"3:1 vs 2:1: {diff_3v2_direct}\")\nprint(\"================================================\")\nprint(\"From regression coefficients: \\n\")\nprint(f\"2:1 vs 1:1: {coef_2v1_reg}\")\nprint(f\"3:1 vs 2:1: {coef_3v2_reg}\")\n\nDirect from data: \n\n2:1 vs 1:1: 0.0018842510217149944\n3:1 vs 2:1: 0.00010002398025293902\n================================================\nFrom regression coefficients: \n\n2:1 vs 1:1: 0.0018842510217151158\n3:1 vs 2:1: 0.00010002398025313504\n\n\nTo assess whether larger match ratios increase the likelihood of donating, I calculated the differences in response rates both directly from the data and from regression coefficients. The results were nearly identical across both methods:\n\nThe difference between 2:1 and 1:1 was about 0.19 percentage points.\nThe difference between 3:1 and 2:1 was effectively zero.\nThe difference between 3:1 and 1:1 was again about 0.20 percentage points.\n\nThese findings confirm that while moving from a 1:1 to a 2:1 or 3:1 match may result in a small increase in donation likelihood, the differences are minimal and statistically weak. This supports the paper’s conclusion that larger match ratios do not meaningfully improve response rates beyond the effect of having a match at all.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\ndf_amount = df[['amount', 'treatment', 'control']].dropna()\n\ntreatment = df_amount[df_amount['treatment'] == 1]['amount']\ncontrol = df_amount[df_amount['control'] == 1]['amount']\nt_stat, p_value = stats.ttest_ind(treatment, control, equal_var=False)\nprint('T-test Results: ')\nprint('_______________________________')\nprint(f'T-statistic: {t_stat}\\nP-Value: {p_value}')\n\nT-test Results: \n_______________________________\nT-statistic: 1.9182618934467577\nP-Value: 0.055085665289183336\n\n\nI conducted a t-test to compare average donation amounts between the treatment and control groups. The test produced a t-statistic of 1.92 and a p-value of 0.055, which is just above the conventional 5 percent significance threshold. This suggests a weak, but not statistically significant, indication that the treatment may have increased donation amounts. While the result hints at a possible effect, it is not strong enough to draw a firm conclusion about the impact of matched donations on how much people give.\n\ndf_positive = df[df['amount'] &gt; 0]\n\ntreatment = df_positive[df_positive['treatment'] == 1]['amount']\ncontrol = df_positive[df_positive['control'] == 1]['amount']\nt_stat, p_value = stats.ttest_ind(treatment, control, equal_var=False)\nprint('T-test Results: ')\nprint('_______________________________')\nprint(f'T-statistic: {t_stat}\\nP-Value: {p_value}')\n\nT-test Results: \n_______________________________\nT-statistic: -0.5846089794983359\nP-Value: 0.5590471865673547\n\n\nTo analyze how much people donate conditional on giving, I restricted the data to respondents who made a donation and ran a t-test comparing donation amounts between treatment and control groups. The t-test produced a t-statistic of -0.58 and a p-value of 0.56, indicating no statistically significant difference in donation amounts. This suggests that while matched donations may influence whether someone gives, they do not affect how much donors give once they’ve decided to contribute. Because treatment was randomly assigned, the coefficient has a causal interpretation, but in this case, the effect size is negligible.\n\ndf_donated = df[df['amount'] &gt; 0]\n\ntreatment_donors = df_donated[df_donated['treatment'] == 1]['amount']\ncontrol_donors = df_donated[df_donated['control'] == 1]['amount']\n\ntreatment_mean = treatment_donors.mean()\ncontrol_mean = control_donors.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Treatment histogram\naxes[0].hist(treatment_donors, bins=30, edgecolor='black')\naxes[0].axvline(treatment_mean, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Treatment Group')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].annotate(f'Mean = ${treatment_mean:.2f}', xy=(treatment_mean, 10),\n                 xytext=(treatment_mean + 10, 20), arrowprops=dict(facecolor='red', arrowstyle='-&gt;'))\n\n# Control histogram\naxes[1].hist(control_donors, bins=30, edgecolor='black', color='orange')\naxes[1].axvline(control_mean, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Control Group')\naxes[1].set_xlabel('Donation Amount')\naxes[1].annotate(f'Mean = ${control_mean:.2f}', xy=(control_mean, 10),\n                 xytext=(control_mean + 10, 20), arrowprops=dict(facecolor='red', arrowstyle='-&gt;'))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms show the distribution of donation amounts among individuals who gave, separated by treatment group. Both distributions are right-skewed, with most donations concentrated at lower amounts. The red dashed lines mark the mean donation in each group: $43.87 for treatment and $45.54 for control. The similarity in means visually confirms earlier statistical results, indicating that while the presence of a match may influence whether someone donates, it does not significantly affect how much they give once they’ve decided to contribute."
  },
  {
    "objectID": "blog/hw1/hw1_questions.html#simulation-experiment",
    "href": "blog/hw1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nnp.random.seed(42)\n\n# Control group: Bernoulli(p=0.018), 100,000 draws\ncontrol_sim = np.random.binomial(n=1, p=0.018, size=100000)\n\n# Treatment group: Bernoulli(p=0.022), 10,000 draws\ntreatment_sim = np.random.binomial(n=1, p=0.022, size=10000)\n\n# diff_vector = treatment_sim - np.random.choice(control_sim, size=10000)\ndiff_vector = treatment_sim - control_sim[:10000]\n\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, len(diff_vector) + 1)\n\ntrue_diff = 0.022 - 0.018\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(y=true_diff, color='red', linestyle='--', label=f'True Difference = {true_diff:.3f}')\nplt.title('Law of Large Numbers: Cumulative Avg of Bernoulli Differences (Treatment - Control)')\nplt.xlabel('Number of Simulated Samples')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot illustrates the Law of Large Numbers using simulated donation data. I calculated the cumulative average difference in donation rates between treatment (2.2 percent) and control (1.8 percent) groups across 10,000 simulated comparisons. The blue line shows how the average difference stabilizes, while the red dashed line marks the true difference (0.004). As more samples accumulate, the cumulative average converges to the true value, confirming that larger samples yield more reliable estimates.\n\n\nCentral Limit Theorem\n\nsample_sizes = [50, 200, 500, 1000]\nsimulations = 1000\np_control = 0.018\np_treatment = 0.022\n\ndiff_distributions = {}\n\nnp.random.seed(42)\n\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(simulations):\n        control_draw = np.random.binomial(1, p_control, n)\n        treatment_draw = np.random.binomial(1, p_treatment, n)\n        mean_diff = treatment_draw.mean() - control_draw.mean()\n        diffs.append(mean_diff)\n    diff_distributions[n] = diffs\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(diff_distributions[n], bins=30, edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference (Treatment - Control)\")\n    axes[i].axvline(p_treatment - p_control, color='red', linestyle='--', label='True Difference')\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show the distribution of average differences in donation rates between treatment and control groups across 1,000 simulations at sample sizes of 50, 200, 500, and 1000. At smaller sizes, the distributions are wide and zero (red) is near the center, reflecting high uncertainty. As the sample size grows, the distributions narrow and more centered around the true difference (black) and zero (red) moves toward the tail, making it less likely. This demonstrates the Central Limit Theorem and shows that larger samples improve our ability to detect small treatment effects."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html",
    "href": "blog/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe start by reading in the data from blueprinty.csv.\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe then compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\ncustomers = df[df['iscustomer'] == 1]\nnon_customers = df[df['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=30, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=30, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n\n\n\n\n\n\n\n\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n\n\nCustomers tend to have a slightly higher frequency of companies with more patents compared to non-customers. The distribution for both groups is right-skewed, but the customer group has a longer tail toward higher patent counts. The average patents for customers is around 4.13 and the average patents for non-customers is around 3.47.\nThis suggests that customer companies, on average, have more patents than non-customers, possibly indicating greater innovation or R&D activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nWe also then compare regions and ages by customer status.\n\nimport seaborn as sns\ncustomers = df[df['iscustomer'] == 1]\nnon_customers = df[df['iscustomer'] == 0]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.boxplot(data=df, x='iscustomer', y='age', ax=axes[0])\naxes[0].set_title('Age by Customer Status')\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels(['Non-Customers', 'Customers'])\naxes[0].set_ylabel('Age')\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'])\nsorted_regions = region_counts[1].sort_values(ascending=False).index\nregion_counts_sorted = region_counts.loc[sorted_regions]\nregion_counts_sorted.plot(kind='bar', stacked=False, ax=axes[1])\naxes[1].set_title('Region Distribution by Customer Status')\naxes[1].legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\naxes[1].set_ylabel('Counts')\n\nplt.tight_layout()\nplt.show()\n\nmean_age_customers = customers['age'].mean()\nmean_age_non_customers = non_customers['age'].mean()\n\ncustomer_region_counts = region_counts[1].sort_values(ascending=False)\n\nprint(f'Customer Mean Age: {mean_age_customers}')\nprint(f'Non-Customer Mean Age:{mean_age_non_customers}')\nprint(f'Customer Region Counts:')\nprint('===============================')\ncustomer_region_counts\n\n\n\n\n\n\n\n\nCustomer Mean Age: 26.9002079002079\nNon-Customer Mean Age:26.101570166830225\nCustomer Region Counts:\n===============================\n\n\nregion\nNortheast    328\nSouthwest     52\nMidwest       37\nSouth         35\nNorthwest     29\nName: 1, dtype: int64\n\n\nCustomers tend to be slightly older than non-customers, with a mean age of 26.9 compared to 26.1. The age distributions are similar overall, but customer firms show a slightly higher median and greater variability. Regionally, the Northeast stands out with the highest number of customer firms (328), while other regions such as the Southwest, Midwest, South, and Northwest have significantly fewer customers. This suggests that both firm age and geographic location may be associated with customer status, with the Northeast possibly representing a key market area.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that ( Y_1, Y_2, , Y_n () ). The probability mass function for each observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence, the likelihood function for the sample is:\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod Y_i!}\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nThe function poisson_loglikelihood(lambda_, Y) calculates the log-likelihood of observing a dataset of counts (Y) under a Poisson distribution with rate parameter \\(\\lambda\\). It assumes that the counts are independent and identically distributed. The function first checks whether \\(\\lambda\\) is positive, since the Poisson rate must be greater than zero, and returns negative infinity if it is not. It then computes the log-likelihood by summing the terms (Y (\\(\\lambda\\)) - \\(\\lambda\\) - (Y!)) across all observations. The gammaln(Y + 1) function is used to compute ((Y!)) in a numerically stable way. This implementation is useful for estimating \\(\\lambda\\) using maximum likelihood estimation.\nWe then use our function poisson_loglikelihood(lambda_, Y) to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\nY = df['patents']\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood vs. Lambda')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the log likelihood function of a Poisson model for a range of \\(\\lambda\\) values, using the observed number of patents as the input data. The horizontal axis represents different values of \\(\\lambda\\), the rate parameter of the Poisson distribution, while the vertical axis shows the corresponding log likelihood values. The curve peaks at the value of \\(\\lambda\\) that best fits the data, which is the maximum likelihood estimate (MLE). The shape of the curve illustrates how sensitive the likelihood is to changes in \\(\\lambda\\), with values that are too low or too high producing a poorer fit. The MLE seems be around a \\(\\lambda\\) value of 3.5.\nDeriving the MLE for the Poisson Rate Parameter λ Given that the log-likelihood function is: \\[ \\ell(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right) \\] Take the derivative with respect to \\(\\lambda\\): \\[ \\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n \\] Set the derivative equal to 0: \\[ \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n = 0 \\] Solve for \\(\\lambda\\): \\[ \\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y} \\] Therefore, the maximum likelihood estimator (MLE) for \\(\\lambda\\) is the sample mean: \\[ \\lambda_{\\text{MLE}} = \\bar{Y} \\]\n\nfrom scipy.optimize import minimize\n\ndef neg_loglikelihood(lambda_, Y):\n    return -poisson_loglikelihood(lambda_, Y)\n\ninitial_guess = [1.0]\n\nresult = minimize(neg_loglikelihood, x0=initial_guess, args=(Y,), bounds=[(0.01, None)])\n\nlambda_mle = result.x[0]\nprint(f'Estimated MLE: {lambda_mle}')\n\nEstimated MLE: 3.6846667021660804\n\n\nThe optimized MLE is approximately 3.68.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    # ensure shapes\n    beta = np.asarray(beta).ravel()\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n\n    # linear predictor\n    linpred = X.dot(beta)\n    linpred = np.clip(linpred, -100, 100)   # avoid overflow\n\n    # use math.exp for each element\n    mu = np.array([math.exp(val) for val in linpred])\n\n    if np.any(mu &lt;= 0) or np.any(np.isnan(mu)):\n        return -np.inf\n\n    return np.sum(Y * np.log(mu) - mu - gammaln(Y + 1))\n\nThis function poisson_regression_loglikelihood(beta, Y, X) computes the log-likelihood for a Poisson regression model. Instead of assuming a constant rate \\(\\lambda\\), it models the rate for each observation as \\(\\lambda_i = \\exp(X_i' \\beta)\\), where \\(X_i\\) represents the covariates (such as age, region, and customer status) and \\(\\beta\\) is the vector of coefficients. The function first calculates the linear predictor \\(X \\beta\\), exponentiates it to obtain \\(\\lambda_i\\), and then evaluates the log-likelihood by summing \\(Y_i \\log(lambda_i) - \\lambda_i - \\log(Y_i!)\\) across all observations. This approach allows the expected count to vary across firms based on their characteristics.\nWe then use our function along with Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\ndf['age_squared'] = df['age'] ** 2\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nY = df['patents'].values\nX_matrix = X.values\n\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_loglikelihood, x0=initial_beta, args=(Y, X_matrix), method='BFGS')\n\nbeta_mle = result.x\nhess_inv = result.hess_inv\n\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\nstd_errors = np.sqrt(np.diag(hess_inv))\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509991\n0.439064\n\n\nage\n0.148706\n0.035334\n\n\nage_squared\n-0.002972\n0.000681\n\n\niscustomer\n0.207609\n0.028506\n\n\nNortheast\n0.029155\n0.034799\n\n\nNorthwest\n-0.017578\n0.045014\n\n\nSouth\n0.056565\n0.043264\n\n\nSouthwest\n0.050567\n0.035334\n\n\n\n\n\n\n\nWe then check our results with Python’s sm.GLM() function.\n\nimport statsmodels.api as sm\n\nX_numeric = X.astype(float)\nY_numeric = Y.astype(float)\n\npoisson_model = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\nimport pandas as pd\nresult_table = pd.DataFrame({\n    'coef': poisson_results.params,\n    'std_err': poisson_results.bse\n})\nprint(result_table)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        22:43:16   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n                 coef   std_err\nintercept   -0.508920  0.183179\nage          0.148619  0.013869\nage_squared -0.002970  0.000258\niscustomer   0.207591  0.030895\nNortheast    0.029170  0.043625\nNorthwest   -0.017575  0.053781\nSouth        0.056561  0.052662\nSouthwest    0.050576  0.047198\n\n\nAge has a strong nonlinear relationship with patent counts: each additional year of firm age increases the expected log count (coefficient 0.149, p &lt; .001), but the negative age squared term (coefficient –0.003, p &lt; .001) means that this benefit tapers off around 25 years of age before declining. Firms that are Blueprinty customers produce about 23 percent more patents than non-customers (exp(0.208)≈1.23, p &lt; .001), all else equal. Once age and customer status are accounted for, none of the regions—Northeast, Northwest, South, or Southwest—differs significantly from the Midwest baseline. The model’s Cragg & Uhler pseudo R² of 0.136 indicates these predictors explain roughly 13.6 percent of the variation in patent counts.\n\nX_base = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared']],\n    region_dummies\n], axis=1)\n\nX_0 = X_base.copy()\nX_0['iscustomer'] = 0\nX_0 = X_0[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nX_1 = X_base.copy()\nX_1['iscustomer'] = 1\nX_1 = X_1[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nX_full = X_base.copy()\nX_full['iscustomer'] = df['iscustomer']\nX_full = X_full[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nY = df['patents'].astype(float)\n\nmodel = sm.GLM(Y, X_full.astype(float), family=sm.families.Poisson())\nresult = model.fit()\n\ny_pred_0 = result.predict(X_0.astype(float))\ny_pred_1 = result.predict(X_1.astype(float))\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\naverage_effect\n\nnp.float64(0.7927680710452699)\n\n\nThe analysis shows that, on average, firms predicted to be Blueprinty customers are expected to produce approximately 0.79 more patents than if they were not customers, holding all other firm characteristics constant."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nWe start by reading in the data from blueprinty.csv.\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nWe then compare histograms and means of number of patents by customer status.\n\nimport matplotlib.pyplot as plt\n\ncustomers = df[df['iscustomer'] == 1]\nnon_customers = df[df['iscustomer'] == 0]\n\nplt.figure(figsize=(10, 5))\nplt.hist(customers['patents'], bins=30, alpha=0.6, label='Customers', edgecolor='black')\nplt.hist(non_customers['patents'], bins=30, alpha=0.6, label='Non-Customers', edgecolor='black')\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.title('Histogram of Number of Patents by Customer Status')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nmean_customers = customers['patents'].mean()\nmean_non_customers = non_customers['patents'].mean()\n\nprint(f'Customer Mean Number of Patents: {mean_customers}')\nprint(f'Non-Customer Mean Number of Patents: {mean_non_customers}')\n\n\n\n\n\n\n\n\nCustomer Mean Number of Patents: 4.133056133056133\nNon-Customer Mean Number of Patents: 3.4730127576054954\n\n\nCustomers tend to have a slightly higher frequency of companies with more patents compared to non-customers. The distribution for both groups is right-skewed, but the customer group has a longer tail toward higher patent counts. The average patents for customers is around 4.13 and the average patents for non-customers is around 3.47.\nThis suggests that customer companies, on average, have more patents than non-customers, possibly indicating greater innovation or R&D activity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nWe also then compare regions and ages by customer status.\n\nimport seaborn as sns\ncustomers = df[df['iscustomer'] == 1]\nnon_customers = df[df['iscustomer'] == 0]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.boxplot(data=df, x='iscustomer', y='age', ax=axes[0])\naxes[0].set_title('Age by Customer Status')\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels(['Non-Customers', 'Customers'])\naxes[0].set_ylabel('Age')\n\nregion_counts = pd.crosstab(df['region'], df['iscustomer'])\nsorted_regions = region_counts[1].sort_values(ascending=False).index\nregion_counts_sorted = region_counts.loc[sorted_regions]\nregion_counts_sorted.plot(kind='bar', stacked=False, ax=axes[1])\naxes[1].set_title('Region Distribution by Customer Status')\naxes[1].legend(title='Customer Status', labels=['Non-Customer', 'Customer'])\naxes[1].set_ylabel('Counts')\n\nplt.tight_layout()\nplt.show()\n\nmean_age_customers = customers['age'].mean()\nmean_age_non_customers = non_customers['age'].mean()\n\ncustomer_region_counts = region_counts[1].sort_values(ascending=False)\n\nprint(f'Customer Mean Age: {mean_age_customers}')\nprint(f'Non-Customer Mean Age:{mean_age_non_customers}')\nprint(f'Customer Region Counts:')\nprint('===============================')\ncustomer_region_counts\n\n\n\n\n\n\n\n\nCustomer Mean Age: 26.9002079002079\nNon-Customer Mean Age:26.101570166830225\nCustomer Region Counts:\n===============================\n\n\nregion\nNortheast    328\nSouthwest     52\nMidwest       37\nSouth         35\nNorthwest     29\nName: 1, dtype: int64\n\n\nCustomers tend to be slightly older than non-customers, with a mean age of 26.9 compared to 26.1. The age distributions are similar overall, but customer firms show a slightly higher median and greater variability. Regionally, the Northeast stands out with the highest number of customer firms (328), while other regions such as the Southwest, Midwest, South, and Northwest have significantly fewer customers. This suggests that both firm age and geographic location may be associated with customer status, with the Northeast possibly representing a key market area.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that ( Y_1, Y_2, , Y_n () ). The probability mass function for each observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independence, the likelihood function for the sample is:\n\\[\nL(\\lambda; Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis simplifies to:\n\\[\nL(\\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum Y_i}}{\\prod Y_i!}\n\\]\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lambda_, Y):\n    if lambda_ &lt;= 0:\n        return -np.inf  \n    return np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))\n\nThe function poisson_loglikelihood(lambda_, Y) calculates the log-likelihood of observing a dataset of counts (Y) under a Poisson distribution with rate parameter \\(\\lambda\\). It assumes that the counts are independent and identically distributed. The function first checks whether \\(\\lambda\\) is positive, since the Poisson rate must be greater than zero, and returns negative infinity if it is not. It then computes the log-likelihood by summing the terms (Y (\\(\\lambda\\)) - \\(\\lambda\\) - (Y!)) across all observations. The gammaln(Y + 1) function is used to compute ((Y!)) in a numerically stable way. This implementation is useful for estimating \\(\\lambda\\) using maximum likelihood estimation.\nWe then use our function poisson_loglikelihood(lambda_, Y) to plot lambda on the horizontal axis and the log-likelihood on the vertical axis for a range of lambdas.\n\nY = df['patents']\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambda_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood')\nplt.title('Poisson Log-Likelihood vs. Lambda')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the log likelihood function of a Poisson model for a range of \\(\\lambda\\) values, using the observed number of patents as the input data. The horizontal axis represents different values of \\(\\lambda\\), the rate parameter of the Poisson distribution, while the vertical axis shows the corresponding log likelihood values. The curve peaks at the value of \\(\\lambda\\) that best fits the data, which is the maximum likelihood estimate (MLE). The shape of the curve illustrates how sensitive the likelihood is to changes in \\(\\lambda\\), with values that are too low or too high producing a poorer fit. The MLE seems be around a \\(\\lambda\\) value of 3.5.\nDeriving the MLE for the Poisson Rate Parameter λ Given that the log-likelihood function is: \\[ \\ell(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right) \\] Take the derivative with respect to \\(\\lambda\\): \\[ \\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n \\] Set the derivative equal to 0: \\[ \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n = 0 \\] Solve for \\(\\lambda\\): \\[ \\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y} \\] Therefore, the maximum likelihood estimator (MLE) for \\(\\lambda\\) is the sample mean: \\[ \\lambda_{\\text{MLE}} = \\bar{Y} \\]\n\nfrom scipy.optimize import minimize\n\ndef neg_loglikelihood(lambda_, Y):\n    return -poisson_loglikelihood(lambda_, Y)\n\ninitial_guess = [1.0]\n\nresult = minimize(neg_loglikelihood, x0=initial_guess, args=(Y,), bounds=[(0.01, None)])\n\nlambda_mle = result.x[0]\nprint(f'Estimated MLE: {lambda_mle}')\n\nEstimated MLE: 3.6846667021660804\n\n\nThe optimized MLE is approximately 3.68.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    # ensure shapes\n    beta = np.asarray(beta).ravel()\n    X = np.asarray(X)\n    Y = np.asarray(Y)\n\n    # linear predictor\n    linpred = X.dot(beta)\n    linpred = np.clip(linpred, -100, 100)   # avoid overflow\n\n    # use math.exp for each element\n    mu = np.array([math.exp(val) for val in linpred])\n\n    if np.any(mu &lt;= 0) or np.any(np.isnan(mu)):\n        return -np.inf\n\n    return np.sum(Y * np.log(mu) - mu - gammaln(Y + 1))\n\nThis function poisson_regression_loglikelihood(beta, Y, X) computes the log-likelihood for a Poisson regression model. Instead of assuming a constant rate \\(\\lambda\\), it models the rate for each observation as \\(\\lambda_i = \\exp(X_i' \\beta)\\), where \\(X_i\\) represents the covariates (such as age, region, and customer status) and \\(\\beta\\) is the vector of coefficients. The function first calculates the linear predictor \\(X \\beta\\), exponentiates it to obtain \\(\\lambda_i\\), and then evaluates the log-likelihood by summing \\(Y_i \\log(lambda_i) - \\lambda_i - \\log(Y_i!)\\) across all observations. This approach allows the expected count to vary across firms based on their characteristics.\nWe then use our function along with Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\ndf['age_squared'] = df['age'] ** 2\nregion_dummies = pd.get_dummies(df['region'], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\nY = df['patents'].values\nX_matrix = X.values\n\ndef neg_loglikelihood(beta, Y, X):\n    return -poisson_regression_loglikelihood(beta, Y, X)\n\ninitial_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_loglikelihood, x0=initial_beta, args=(Y, X_matrix), method='BFGS')\n\nbeta_mle = result.x\nhess_inv = result.hess_inv\n\nif not isinstance(hess_inv, np.ndarray):\n    hess_inv = hess_inv.todense()\nhess_inv = np.asarray(hess_inv)\n\nstd_errors = np.sqrt(np.diag(hess_inv))\n\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": std_errors\n}, index=X.columns)\nresults_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509991\n0.439064\n\n\nage\n0.148706\n0.035334\n\n\nage_squared\n-0.002972\n0.000681\n\n\niscustomer\n0.207609\n0.028506\n\n\nNortheast\n0.029155\n0.034799\n\n\nNorthwest\n-0.017578\n0.045014\n\n\nSouth\n0.056565\n0.043264\n\n\nSouthwest\n0.050567\n0.035334\n\n\n\n\n\n\n\nWe then check our results with Python’s sm.GLM() function.\n\nimport statsmodels.api as sm\n\nX_numeric = X.astype(float)\nY_numeric = Y.astype(float)\n\npoisson_model = sm.GLM(Y_numeric, X_numeric, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nprint(poisson_results.summary())\n\nimport pandas as pd\nresult_table = pd.DataFrame({\n    'coef': poisson_results.params,\n    'std_err': poisson_results.bse\n})\nprint(result_table)\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Wed, 07 May 2025   Deviance:                       2143.3\nTime:                        22:43:16   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nintercept      -0.5089      0.183     -2.778      0.005      -0.868      -0.150\nage             0.1486      0.014     10.716      0.000       0.121       0.176\nage_squared    -0.0030      0.000    -11.513      0.000      -0.003      -0.002\niscustomer      0.2076      0.031      6.719      0.000       0.147       0.268\nNortheast       0.0292      0.044      0.669      0.504      -0.056       0.115\nNorthwest      -0.0176      0.054     -0.327      0.744      -0.123       0.088\nSouth           0.0566      0.053      1.074      0.283      -0.047       0.160\nSouthwest       0.0506      0.047      1.072      0.284      -0.042       0.143\n===============================================================================\n                 coef   std_err\nintercept   -0.508920  0.183179\nage          0.148619  0.013869\nage_squared -0.002970  0.000258\niscustomer   0.207591  0.030895\nNortheast    0.029170  0.043625\nNorthwest   -0.017575  0.053781\nSouth        0.056561  0.052662\nSouthwest    0.050576  0.047198\n\n\nAge has a strong nonlinear relationship with patent counts: each additional year of firm age increases the expected log count (coefficient 0.149, p &lt; .001), but the negative age squared term (coefficient –0.003, p &lt; .001) means that this benefit tapers off around 25 years of age before declining. Firms that are Blueprinty customers produce about 23 percent more patents than non-customers (exp(0.208)≈1.23, p &lt; .001), all else equal. Once age and customer status are accounted for, none of the regions—Northeast, Northwest, South, or Southwest—differs significantly from the Midwest baseline. The model’s Cragg & Uhler pseudo R² of 0.136 indicates these predictors explain roughly 13.6 percent of the variation in patent counts.\n\nX_base = pd.concat([\n    pd.Series(1, index=df.index, name='intercept'),\n    df[['age', 'age_squared']],\n    region_dummies\n], axis=1)\n\nX_0 = X_base.copy()\nX_0['iscustomer'] = 0\nX_0 = X_0[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nX_1 = X_base.copy()\nX_1['iscustomer'] = 1\nX_1 = X_1[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nX_full = X_base.copy()\nX_full['iscustomer'] = df['iscustomer']\nX_full = X_full[['intercept', 'age', 'age_squared', 'iscustomer'] + list(region_dummies.columns)]\n\nY = df['patents'].astype(float)\n\nmodel = sm.GLM(Y, X_full.astype(float), family=sm.families.Poisson())\nresult = model.fit()\n\ny_pred_0 = result.predict(X_0.astype(float))\ny_pred_1 = result.predict(X_1.astype(float))\n\naverage_effect = np.mean(y_pred_1 - y_pred_0)\naverage_effect\n\nnp.float64(0.7927680710452699)\n\n\nThe analysis shows that, on average, firms predicted to be Blueprinty customers are expected to produce approximately 0.79 more patents than if they were not customers, holding all other firm characteristics constant."
  },
  {
    "objectID": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "href": "blog/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nEDA:\n\ndf = pd.read_csv(\"airbnb.csv\")\ndf.describe(include='all')\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\ncount\n40628.000000\n4.062800e+04\n40628.000000\n40628\n40593\n40628\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n40628\n\n\nunique\nNaN\nNaN\nNaN\n2\n2790\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\ntop\nNaN\nNaN\nNaN\n4/2/2017\n12/21/2015\nEntire home/apt\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nf\n\n\nfreq\nNaN\nNaN\nNaN\n25737\n65\n19873\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32759\n\n\nmean\n20314.500000\n9.698889e+06\n1102.368219\nNaN\nNaN\nNaN\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\nNaN\n\n\nstd\n11728.437705\n5.460166e+06\n1383.269358\nNaN\nNaN\nNaN\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\nNaN\n\n\nmin\n1.000000\n2.515000e+03\n1.000000\nNaN\nNaN\nNaN\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\nNaN\n\n\n25%\n10157.750000\n4.889868e+06\n542.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\nNaN\n\n\n50%\n20314.500000\n9.862878e+06\n996.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n75%\n30471.250000\n1.466789e+07\n1535.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\nmax\n40628.000000\n1.800967e+07\n42828.000000\nNaN\nNaN\nNaN\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n\n\n\n\n\n\nmissing_values = df.isna().sum()\n\nmissing_values = missing_values[missing_values &gt; 0]\n\nplt.figure(figsize=(10, 6))\nmissing_values.plot(kind='bar')\nplt.title('Missing Values per Column')\nplt.xlabel('Columns')\nplt.ylabel('Number of Missing Values')\nplt.xticks(rotation=45)\nplt.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(\"Missing Values:\")\nprint(missing_values)\n\n\n\n\n\n\n\n\nMissing Values:\nhost_since                      35\nbathrooms                      160\nbedrooms                        76\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ndtype: int64\n\n\n\nhost_since: 35 missing\nbathrooms: 160 missing\nbedrooms: 76 missing\ncleanliness: 10,195 missing\nlocation: 10,254 missing\nvalue: 10,256 missing\n\nWe then built a poisson regression model for the number of bookings as proxied by the number of reviews.\n\ncolumns_required = [\n    'days', 'room_type', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable', 'number_of_reviews'\n]\ndf_clean = df.dropna(subset=columns_required)\n\ndf_clean = pd.get_dummies(df_clean, columns=['room_type', 'instant_bookable'], drop_first=True)\n\nX = df_clean[[\n    'days', 'bathrooms', 'bedrooms', 'price',\n    'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n    'room_type_Private room', 'room_type_Shared room', 'instant_bookable_t'\n]]\nX = sm.add_constant(X)  \nY = df_clean['number_of_reviews']\n\nX = X.astype(float)\nY = Y.astype(float)\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nresults_df = pd.DataFrame({\n    'Coefficient': result.params,\n    'Std. Error': result.bse\n})\nprint(results_df)\n\n                           Coefficient    Std. Error\nconst                         3.498049  1.609066e-02\ndays                          0.000051  3.909218e-07\nbathrooms                    -0.117704  3.749225e-03\nbedrooms                      0.074087  1.991742e-03\nprice                        -0.000018  8.326458e-06\nreview_scores_cleanliness     0.113139  1.496336e-03\nreview_scores_location       -0.076899  1.608903e-03\nreview_scores_value          -0.091076  1.803855e-03\nroom_type_Private room       -0.010536  2.738448e-03\nroom_type_Shared room        -0.246337  8.619793e-03\ninstant_bookable_t            0.345850  2.890138e-03\n\n\nWe dropped rows with missing values for modeling, created dummy variables for room_type and instant_bookable, and fit a Poisson regression model with number_of_reviews as the outcome.\nObservations: - Intercept (3.50): - This is the expected log number of reviews for a listing with all predictors at zero. While not directly interpretable on its own, it anchors the model.\n\ndays (0.0000507):\n\nFor each additional day a listing is on the platform, the expected number of reviews increases slightly. Since the coefficient is small, the cumulative effect builds over time.\n\nbathrooms (–0.118):\n\nMore bathrooms are associated with fewer reviews, holding everything else constant. This may indicate that listings with more bathrooms (likely larger or higher-end) receive fewer but possibly higher-value bookings.\n\nbedrooms (0.074):\n\nEach additional bedroom is associated with more reviews, suggesting larger units are booked more frequently.\n\nprice (–0.0000179):\n\nHigher nightly prices are associated with fewer reviews, as expected. Though small per dollar, the effect accumulates for expensive listings.\n\nCleanliness (0.113):\n\nHigher cleanliness scores significantly increase the number of reviews, reflecting the value guests place on a clean space.\n\nValue (–0.077) and Location (–0.091):\n\nInterestingly, higher value and location scores are slightly negatively associated with review counts, possibly reflecting more niche or stable listings that receive fewer but more favorable reviews.\n\nPrivate Room (–0.0105):\n\nCompared to entire homes, private rooms receive slightly fewer reviews.\n\nShared Room (–0.247):\n\nShared rooms receive significantly fewer reviews, suggesting they are much less popular among guests."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html",
    "href": "blog/hw3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/hw3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/hw3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)  # From 8 to 32 inclusive, step 4\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brands for a in ads for p in prices],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Parameters for the simulation\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent's data\ndef simulate_one_respondent(resp_id):\n    records = []\n    \n    for t in range(1, n_tasks + 1):\n        # Sample 3 random alternatives\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = resp_id\n        sampled[\"task\"] = t\n\n        # Compute deterministic utility\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util) +\n            sampled[\"ad\"].map(a_util) +\n            sampled[\"price\"].apply(p_util)\n        )\n\n        # Add Gumbel noise (Type I extreme value)\n        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Identify chosen alternative (1 = chosen, 0 = not)\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        records.append(sampled[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]])\n\n    return pd.concat(records, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat(\n    [simulate_one_respondent(i) for i in range(1, n_peeps + 1)],\n    ignore_index=True\n)\n\n# Display first few rows\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/hw3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data\n\nimport pandas as pd\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\n\n\n\n\n0\n1\n1\n1\nN\nYes\n28\n\n\n1\n1\n1\n0\nH\nYes\n16\n\n\n2\n1\n1\n0\nP\nYes\n16\n\n\n3\n1\n2\n0\nN\nYes\n32\n\n\n4\n1\n2\n1\nP\nYes\n16\n\n\n\n\n\n\n\n\ndf = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\ndf.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nbrand_N\nbrand_P\nad_Yes\n\n\n\n\n0\n1\n1\n1\n28\nTrue\nFalse\nTrue\n\n\n1\n1\n1\n0\n16\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n0\n16\nFalse\nTrue\nTrue\n\n\n3\n1\n2\n0\n32\nTrue\nFalse\nTrue\n\n\n4\n1\n2\n1\n16\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n# Design matrix X\nX = df[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].copy()\n\n# Response vector y\ny = df[\"choice\"].copy()\n\ndf[\"choice_id\"] = df[\"resp\"].astype(str) + \"_\" + df[\"task\"].astype(str)\nle = LabelEncoder()\ndf[\"choice_id\"] = le.fit_transform(df[\"choice_id\"])\n\nprint(X.shape)\nprint(y.sum(), \"choices made\")\nprint(df[\"choice_id\"].nunique(), \"unique tasks\")\n\n(3000, 4)\n1000 choices made\n1000 unique tasks\n\n\nThe dataset contains 3,000 observations from a choice based conjoint survey. Each respondent completed several tasks, each involving three product alternatives, and selected one option per task. There are 1,000 unique tasks, suggesting about 333 respondents each completed three tasks.\nThe dataset includes both categorical and numerical variables. The categorical variables include brand (H, N, P) and ad (Yes or No), which were converted to binary indicators using one hot encoding, with one category dropped to avoid multicollinearity. Price is a numerical variable. The response variable, choice, indicates the selected product in each task, with exactly one choice per task."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/hw3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\ndef negative_log_likelihood(beta, X, y, choice_ids):\n    Xb = X @ beta\n    Xb = np.asarray(Xb, dtype=np.float64)  # &lt;=== Ensure it's numeric\n\n    df = pd.DataFrame({\n        \"Xb\": Xb,\n        \"y\": y,\n        \"choice_id\": choice_ids\n    })\n\n    # Compute softmax denominators per choice set\n    df[\"exp_Xb\"] = np.exp(df[\"Xb\"])\n    denom = df.groupby(\"choice_id\")[\"exp_Xb\"].transform(\"sum\")\n    df[\"prob\"] = df[\"exp_Xb\"] / denom\n\n    # Log likelihood contribution only from chosen alternatives\n    log_likelihood = np.sum(df[\"y\"] * np.log(df[\"prob\"]))\n    \n    return -log_likelihood  # negate because we minimize\n\n\n# Ensure X is numpy array\nX_mat = X.values\ny_vec = y.values\nchoice_ids = df[\"choice_id\"].values\n\n# Initial guess\nbeta_init = np.zeros(X_mat.shape[1])\n\n# Optimize\nresult = minimize(negative_log_likelihood, beta_init, args=(X_mat, y_vec, choice_ids), method='BFGS')\n\n# Estimated betas\nbeta_hat = result.x\n\n\n# Inverse Hessian = estimated variance-covariance matrix\nhess_inv = result.hess_inv\nse = np.sqrt(np.diag(hess_inv))  # Standard errors\n\n# 95% Confidence intervals\nz = 1.96\nconf_int = np.column_stack((beta_hat - z * se, beta_hat + z * se))\n\n# Parameter names\nparam_names = X.columns.tolist()\n\n# Summary\nsummary_df = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se,\n    \"95% CI Lower\": conf_int[:, 0],\n    \"95% CI Upper\": conf_int[:, 1]\n})\n\nsummary_df\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbrand_N\n0.941195\n0.105751\n0.733922\n1.148468\n\n\n1\nbrand_P\n0.501616\n0.101876\n0.301939\n0.701292\n\n\n2\nad_Yes\n-0.731994\n0.088966\n-0.906369\n-0.557620\n\n\n3\nprice\n-0.099480\n0.006366\n-0.111958\n-0.087003\n\n\n\n\n\n\n\nThe maximum likelihood estimation results provide insights into consumer preferences based on the estimated coefficients. The coefficient for brand_N is the highest at 0.94, indicating a strong positive preference relative to the baseline brand. Brand_P also has a positive effect (0.50), though smaller. The presence of ads has a significant negative impact on choice, with a coefficient of -0.73. Price is also negatively associated with choice, as expected, with a coefficient of -0.10. All estimates are statistically significant, with 95% confidence intervals that do not cross zero, confirming the reliability of the effects."
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/hw3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Use the existing negative_log_likelihood from MLE section\ndef log_prior(beta):\n    # First 3: N(0, 25); Last (price): N(0, 1)\n    lp = -0.5 * ((beta[:3] / 5)**2).sum() - 0.5 * (beta[3] / 1)**2\n    return lp  # log prior\n\ndef log_posterior(beta, X, y, choice_ids):\n    return -negative_log_likelihood(beta, X, y, choice_ids) + log_prior(beta)\n\ndef metropolis_hastings(log_posterior, X, y, choice_ids, n_steps=11000, burn=1000):\n    # Initialize\n    current_beta = np.zeros(X.shape[1])\n    current_log_post = log_posterior(current_beta, X, y, choice_ids)\n    \n    samples = []\n    accepted = 0\n\n    for step in tqdm(range(n_steps)):\n        # Propose new beta\n        proposal = current_beta.copy()\n        proposal[:3] += np.random.normal(0, np.sqrt(0.05), size=3)   # Netflix, Prime, Ads\n        proposal[3]  += np.random.normal(0, np.sqrt(0.005))          # Price\n\n        # Compute log-posterior at proposal\n        proposal_log_post = log_posterior(proposal, X, y, choice_ids)\n\n        # Acceptance probability\n        log_accept_ratio = proposal_log_post - current_log_post\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            current_beta = proposal\n            current_log_post = proposal_log_post\n            accepted += 1\n\n        samples.append(current_beta.copy())\n\n    print(f\"Acceptance rate: {accepted / n_steps:.3f}\")\n    return np.array(samples[burn:])  # discard burn-in\n\nsamples = metropolis_hastings(log_posterior, X_mat, y_vec, choice_ids)\n\nparam_names = X.columns.tolist()\n\nposterior_df = pd.DataFrame(samples, columns=param_names)\n\n  0%|          | 0/11000 [00:00&lt;?, ?it/s]  1%|          | 75/11000 [00:00&lt;00:14, 747.23it/s]  1%|▏         | 151/11000 [00:00&lt;00:14, 751.21it/s]  2%|▏         | 228/11000 [00:00&lt;00:14, 758.44it/s]  3%|▎         | 308/11000 [00:00&lt;00:13, 772.90it/s]  4%|▎         | 386/11000 [00:00&lt;00:13, 772.08it/s]  4%|▍         | 469/11000 [00:00&lt;00:13, 789.36it/s]  5%|▍         | 548/11000 [00:00&lt;00:13, 763.23it/s]  6%|▌         | 625/11000 [00:00&lt;00:13, 749.00it/s]  6%|▋         | 701/11000 [00:00&lt;00:13, 747.20it/s]  7%|▋         | 778/11000 [00:01&lt;00:13, 753.90it/s]  8%|▊         | 858/11000 [00:01&lt;00:13, 764.79it/s]  9%|▊         | 943/11000 [00:01&lt;00:12, 786.15it/s]  9%|▉         | 1022/11000 [00:01&lt;00:12, 781.07it/s] 10%|█         | 1103/11000 [00:01&lt;00:12, 788.59it/s] 11%|█         | 1185/11000 [00:01&lt;00:12, 796.04it/s] 12%|█▏        | 1270/11000 [00:01&lt;00:11, 811.82it/s] 12%|█▏        | 1352/11000 [00:01&lt;00:11, 804.69it/s] 13%|█▎        | 1434/11000 [00:01&lt;00:11, 807.16it/s] 14%|█▍        | 1517/11000 [00:01&lt;00:11, 813.84it/s] 15%|█▍        | 1600/11000 [00:02&lt;00:11, 816.95it/s] 15%|█▌        | 1682/11000 [00:02&lt;00:11, 810.74it/s] 16%|█▌        | 1764/11000 [00:02&lt;00:11, 793.32it/s] 17%|█▋        | 1844/11000 [00:02&lt;00:11, 783.32it/s] 18%|█▊        | 1925/11000 [00:02&lt;00:11, 787.90it/s] 18%|█▊        | 2007/11000 [00:02&lt;00:11, 793.99it/s] 19%|█▉        | 2087/11000 [00:02&lt;00:11, 794.46it/s] 20%|█▉        | 2167/11000 [00:02&lt;00:11, 784.29it/s] 20%|██        | 2248/11000 [00:02&lt;00:11, 790.62it/s] 21%|██        | 2335/11000 [00:02&lt;00:10, 811.90it/s] 22%|██▏       | 2417/11000 [00:03&lt;00:10, 802.42it/s] 23%|██▎       | 2498/11000 [00:03&lt;00:10, 789.75it/s] 23%|██▎       | 2578/11000 [00:03&lt;00:10, 779.29it/s] 24%|██▍       | 2656/11000 [00:03&lt;00:10, 771.75it/s] 25%|██▍       | 2738/11000 [00:03&lt;00:10, 784.57it/s] 26%|██▌       | 2820/11000 [00:03&lt;00:10, 794.58it/s] 26%|██▋       | 2900/11000 [00:03&lt;00:10, 785.89it/s] 27%|██▋       | 2980/11000 [00:03&lt;00:10, 789.30it/s] 28%|██▊       | 3061/11000 [00:03&lt;00:09, 794.32it/s] 29%|██▊       | 3141/11000 [00:03&lt;00:09, 789.18it/s] 29%|██▉       | 3221/11000 [00:04&lt;00:09, 791.19it/s] 30%|███       | 3301/11000 [00:04&lt;00:10, 769.82it/s] 31%|███       | 3379/11000 [00:04&lt;00:10, 728.00it/s] 31%|███▏      | 3455/11000 [00:04&lt;00:10, 736.16it/s] 32%|███▏      | 3535/11000 [00:04&lt;00:09, 753.75it/s] 33%|███▎      | 3613/11000 [00:04&lt;00:09, 756.67it/s] 34%|███▎      | 3689/11000 [00:04&lt;00:09, 744.31it/s] 34%|███▍      | 3764/11000 [00:04&lt;00:09, 744.95it/s] 35%|███▌      | 3853/11000 [00:04&lt;00:09, 783.43it/s] 36%|███▌      | 3940/11000 [00:05&lt;00:08, 807.42it/s] 37%|███▋      | 4021/11000 [00:05&lt;00:08, 805.69it/s] 37%|███▋      | 4102/11000 [00:05&lt;00:08, 791.01it/s] 38%|███▊      | 4188/11000 [00:05&lt;00:08, 809.47it/s] 39%|███▉      | 4273/11000 [00:05&lt;00:08, 819.35it/s] 40%|███▉      | 4358/11000 [00:05&lt;00:08, 824.03it/s] 40%|████      | 4442/11000 [00:05&lt;00:07, 826.88it/s] 41%|████      | 4525/11000 [00:05&lt;00:07, 812.14it/s] 42%|████▏     | 4612/11000 [00:05&lt;00:07, 825.00it/s] 43%|████▎     | 4695/11000 [00:05&lt;00:07, 803.42it/s] 43%|████▎     | 4783/11000 [00:06&lt;00:07, 822.43it/s] 44%|████▍     | 4866/11000 [00:06&lt;00:07, 819.32it/s] 45%|████▌     | 4951/11000 [00:06&lt;00:07, 828.27it/s] 46%|████▌     | 5034/11000 [00:06&lt;00:07, 807.39it/s] 46%|████▋     | 5115/11000 [00:06&lt;00:07, 800.16it/s] 47%|████▋     | 5199/11000 [00:06&lt;00:07, 809.87it/s] 48%|████▊     | 5281/11000 [00:06&lt;00:07, 803.24it/s] 49%|████▉     | 5363/11000 [00:06&lt;00:06, 807.00it/s] 50%|████▉     | 5446/11000 [00:06&lt;00:06, 810.36it/s] 50%|█████     | 5528/11000 [00:07&lt;00:07, 779.52it/s] 51%|█████     | 5607/11000 [00:07&lt;00:06, 780.10it/s] 52%|█████▏    | 5686/11000 [00:07&lt;00:06, 779.99it/s] 52%|█████▏    | 5771/11000 [00:07&lt;00:06, 799.67it/s] 53%|█████▎    | 5853/11000 [00:07&lt;00:06, 805.62it/s] 54%|█████▍    | 5937/11000 [00:07&lt;00:06, 815.34it/s] 55%|█████▍    | 6019/11000 [00:07&lt;00:06, 786.44it/s] 55%|█████▌    | 6098/11000 [00:07&lt;00:06, 779.72it/s] 56%|█████▌    | 6182/11000 [00:07&lt;00:06, 795.24it/s] 57%|█████▋    | 6262/11000 [00:07&lt;00:05, 794.10it/s] 58%|█████▊    | 6342/11000 [00:08&lt;00:05, 792.12it/s] 58%|█████▊    | 6422/11000 [00:08&lt;00:05, 784.35it/s] 59%|█████▉    | 6504/11000 [00:08&lt;00:05, 794.75it/s] 60%|█████▉    | 6584/11000 [00:08&lt;00:05, 792.88it/s] 61%|██████    | 6664/11000 [00:08&lt;00:05, 793.50it/s] 61%|██████▏   | 6744/11000 [00:08&lt;00:05, 772.53it/s] 62%|██████▏   | 6822/11000 [00:08&lt;00:05, 771.59it/s] 63%|██████▎   | 6906/11000 [00:08&lt;00:05, 789.62it/s] 64%|██████▎   | 6988/11000 [00:08&lt;00:05, 798.31it/s] 64%|██████▍   | 7069/11000 [00:08&lt;00:04, 799.69it/s] 65%|██████▌   | 7150/11000 [00:09&lt;00:04, 801.66it/s] 66%|██████▌   | 7231/11000 [00:09&lt;00:04, 787.10it/s] 66%|██████▋   | 7310/11000 [00:09&lt;00:04, 787.55it/s] 67%|██████▋   | 7395/11000 [00:09&lt;00:04, 802.99it/s] 68%|██████▊   | 7481/11000 [00:09&lt;00:04, 818.58it/s] 69%|██████▉   | 7564/11000 [00:09&lt;00:04, 821.55it/s] 70%|██████▉   | 7647/11000 [00:09&lt;00:04, 818.34it/s] 70%|███████   | 7729/11000 [00:09&lt;00:04, 810.43it/s] 71%|███████   | 7811/11000 [00:09&lt;00:04, 791.20it/s] 72%|███████▏  | 7891/11000 [00:09&lt;00:03, 789.14it/s] 72%|███████▏  | 7970/11000 [00:10&lt;00:03, 782.45it/s] 73%|███████▎  | 8051/11000 [00:10&lt;00:03, 788.23it/s] 74%|███████▍  | 8130/11000 [00:10&lt;00:03, 787.12it/s] 75%|███████▍  | 8214/11000 [00:10&lt;00:03, 802.19it/s] 75%|███████▌  | 8295/11000 [00:10&lt;00:03, 801.13it/s] 76%|███████▌  | 8376/11000 [00:10&lt;00:03, 778.49it/s] 77%|███████▋  | 8458/11000 [00:10&lt;00:03, 789.36it/s] 78%|███████▊  | 8543/11000 [00:10&lt;00:03, 805.95it/s] 78%|███████▊  | 8629/11000 [00:10&lt;00:02, 821.52it/s] 79%|███████▉  | 8712/11000 [00:11&lt;00:02, 794.99it/s] 80%|███████▉  | 8792/11000 [00:11&lt;00:02, 763.60it/s] 81%|████████  | 8875/11000 [00:11&lt;00:02, 780.53it/s] 81%|████████▏ | 8959/11000 [00:11&lt;00:02, 795.35it/s] 82%|████████▏ | 9039/11000 [00:11&lt;00:02, 791.99it/s] 83%|████████▎ | 9122/11000 [00:11&lt;00:02, 802.71it/s] 84%|████████▎ | 9206/11000 [00:11&lt;00:02, 812.38it/s] 84%|████████▍ | 9291/11000 [00:11&lt;00:02, 823.29it/s] 85%|████████▌ | 9374/11000 [00:11&lt;00:01, 814.40it/s] 86%|████████▌ | 9456/11000 [00:11&lt;00:01, 812.02it/s] 87%|████████▋ | 9538/11000 [00:12&lt;00:01, 797.87it/s] 87%|████████▋ | 9618/11000 [00:12&lt;00:01, 772.44it/s] 88%|████████▊ | 9696/11000 [00:12&lt;00:01, 772.52it/s] 89%|████████▉ | 9784/11000 [00:12&lt;00:01, 803.66it/s] 90%|████████▉ | 9867/11000 [00:12&lt;00:01, 811.01it/s] 90%|█████████ | 9949/11000 [00:12&lt;00:01, 798.91it/s] 91%|█████████ | 10030/11000 [00:12&lt;00:01, 781.80it/s] 92%|█████████▏| 10109/11000 [00:12&lt;00:01, 773.04it/s] 93%|█████████▎| 10192/11000 [00:12&lt;00:01, 786.36it/s] 93%|█████████▎| 10271/11000 [00:12&lt;00:00, 780.70it/s] 94%|█████████▍| 10352/11000 [00:13&lt;00:00, 788.85it/s] 95%|█████████▍| 10431/11000 [00:13&lt;00:00, 784.60it/s] 96%|█████████▌| 10514/11000 [00:13&lt;00:00, 797.13it/s] 96%|█████████▋| 10596/11000 [00:13&lt;00:00, 802.00it/s] 97%|█████████▋| 10679/11000 [00:13&lt;00:00, 808.59it/s] 98%|█████████▊| 10760/11000 [00:13&lt;00:00, 806.36it/s] 99%|█████████▊| 10845/11000 [00:13&lt;00:00, 817.56it/s] 99%|█████████▉| 10928/11000 [00:13&lt;00:00, 818.41it/s]100%|██████████| 11000/11000 [00:13&lt;00:00, 792.99it/s]\n\n\nAcceptance rate: 0.021\n\n\n\n\n\n_todo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\nimport matplotlib.pyplot as plt\n\n# Select parameter\nparam = \"price\"\nsamples_param = posterior_df[param]\n\n# Plot\nplt.figure(figsize=(12, 5))\n\n# Trace plot\nplt.subplot(1, 2, 1)\nplt.plot(samples_param)\nplt.title(f\"Trace Plot for '{param}'\", fontsize=14)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Parameter Value\")\n\n# Histogram\nplt.subplot(1, 2, 2)\nplt.hist(samples_param, bins=30, density=True, alpha=0.75)\nplt.title(f\"Posterior Distribution for '{param}'\", fontsize=14)\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n# import matplotlib.pyplot as plt\n\n# posterior_samples = samples_post  \n\n# beta_idx = 0 \n# param_name = \"Beta_Netflix\"\n\n# plt.figure(figsize=(12, 5))\n\n# plt.subplot(1, 2, 1)\n# plt.plot(posterior_samples[:, beta_idx], color=\"blue\", alpha=0.6)\n# plt.title(f\"Trace Plot: {param_name}\")\n# plt.xlabel(\"Iteration\")\n# plt.ylabel(\"Value\")\n\n# plt.subplot(1, 2, 2)\n# plt.hist(posterior_samples[:, beta_idx], bins=30, color=\"skyblue\", edgecolor=\"black\")\n# plt.title(f\"Posterior Distribution: {param_name}\")\n# plt.xlabel(\"Value\")\n# plt.ylabel(\"Frequency\")\n\n# plt.tight_layout()\n# plt.show()\n\n\n\n\n\n\n\n\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach.\n\nsummary = posterior_df.describe(percentiles=[0.025, 0.5, 0.975]).T[\n    [\"mean\", \"std\", \"2.5%\", \"50%\", \"97.5%\"]\n]\nsummary.columns = [\"Posterior Mean\", \"Std Dev\", \"2.5%\", \"Median\", \"97.5%\"]\nprint(summary)\n\n         Posterior Mean   Std Dev      2.5%    Median     97.5%\nbrand_N        0.916029  0.101983  0.748662  0.919893  1.119922\nbrand_P        0.486399  0.100313  0.305309  0.479714  0.699705\nad_Yes        -0.730533  0.089884 -0.896648 -0.725732 -0.568877\nprice         -0.099519  0.006282 -0.111486 -0.098768 -0.086724"
  },
  {
    "objectID": "blog/hw3/hw3_questions.html#discussion",
    "href": "blog/hw3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\nIf the data had not been simulated and instead came from a real conjoint study, the parameter estimates would reflect actual consumer preferences revealed through their choices. In our case, we observe that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\), which means that, holding all else equal (ads and price), consumers prefer Netflix over Amazon Prime. This reflects higher utility assigned to the Netflix brand relative to Prime. The interpretation of the price coefficient, \\(\\beta_{\\text{price}}\\), being negative is intuitive and expected — as price increases, the utility of a streaming service decreases, making consumers less likely to choose it. This aligns with standard economic theory where higher costs typically reduce demand. Overall, the signs and magnitudes of the estimates are consistent with rational consumer behavior and provide meaningful insights into how brand, ads, and price influence choice.\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\nTo simulate data from and estimate parameters of a multilevel (hierarchical or random parameter) logit model, we would need to allow the part-worth utilities (i.e., the \\(\\beta\\) coefficients) to vary across individuals rather than being fixed for the entire population. This means assuming each individual’s preference vector \\(\\beta\\_i\\) is drawn from a population distribution, typically a multivariate normal distribution with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\). In simulation, we would first draw individual-level \\(\\beta\\_i\\) from this distribution and then use those to generate their choices. For estimation, we would use a Bayesian hierarchical model that includes priors on both the individual-level parameters \\(\\beta\\_i\\) and the hyperparameters \\(\\mu\\) and \\(\\Sigma\\). Estimation would typically require more advanced MCMC techniques like Gibbs sampling or Hamiltonian Monte Carlo (e.g., via Stan or PyMC) since we now have to sample from both the individual-level posteriors and the group-level distributions. This hierarchical structure captures preference heterogeneity across individuals and more accurately reflects real-world data where consumers differ in their sensitivities to product features."
  }
]